{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e72e59",
      "metadata": {
        "id": "92e72e59",
        "outputId": "5c965466-648f-4f86-e83a-9f373732cd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (279, 7)\n",
            "Test shape: (31, 7)\n",
            "Train label distribution:\n",
            " 1    187\n",
            "0     92\n",
            "Name: Class, dtype: int64\n",
            "Test label distribution:\n",
            " 1    23\n",
            "0     8\n",
            "Name: Class, dtype: int64\n",
            "=== Isolation Forest ===\n",
            "Acc=0.4516, Prec=1.0000, Rec=0.2609, F1=0.4138\n",
            "\n",
            "=== One-Class SVM ===\n",
            "Acc=0.6774, Prec=0.9333, Rec=0.6087, F1=0.7368\n",
            "\n",
            "=== Local Outlier Factor ===\n",
            "Acc=0.2903, Prec=1.0000, Rec=0.0435, F1=0.0833\n",
            "\n",
            "=== Simple Autoencoder ===\n",
            "Acc=0.3548, Prec=1.0000, Rec=0.1304, F1=0.2308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Transformer ===\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== LSTM ===\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== MLP ===\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== DeepSVDD ===\n",
            "Acc=0.8065, Prec=0.8400, Rec=0.9130, F1=0.8750\n",
            "\n",
            "========== BASELINE MUTATION ANALYSIS ==========\n",
            "\n",
            "=== Mutation for IsolationForest ===\n",
            "\n",
            "-- IsolationForest A1 Results --\n",
            "Acc=0.4516, Prec=1.0000, Rec=0.2609, F1=0.4138\n",
            "\n",
            "-- IsolationForest A2 Results --\n",
            "Acc=0.4839, Prec=0.8889, Rec=0.3478, F1=0.5000\n",
            "\n",
            "-- IsolationForest A3 Results --\n",
            "Acc=0.3871, Prec=0.7500, Rec=0.2609, F1=0.3871\n",
            "\n",
            "=== Mutation for OneClassSVM ===\n",
            "\n",
            "-- OneClassSVM A1 Results --\n",
            "Acc=0.6452, Prec=0.8750, Rec=0.6087, F1=0.7179\n",
            "\n",
            "-- OneClassSVM A2 Results --\n",
            "Acc=0.7419, Prec=0.9412, Rec=0.6957, F1=0.8000\n",
            "\n",
            "-- OneClassSVM A3 Results --\n",
            "Acc=0.7097, Prec=0.9375, Rec=0.6522, F1=0.7692\n",
            "\n",
            "=== Mutation for LocalOutlierFactor ===\n",
            "\n",
            "-- LocalOutlierFactor A1 Results --\n",
            "Acc=0.3226, Prec=0.7500, Rec=0.1304, F1=0.2222\n",
            "\n",
            "-- LocalOutlierFactor A2 Results --\n",
            "Acc=0.4516, Prec=0.8000, Rec=0.3478, F1=0.4848\n",
            "\n",
            "-- LocalOutlierFactor A3 Results --\n",
            "Acc=0.2903, Prec=1.0000, Rec=0.0435, F1=0.0833\n",
            "\n",
            "=== Mutation for Autoencoder ===\n",
            "\n",
            "-- Autoencoder A1 Results --\n",
            "Acc=0.3871, Prec=0.8333, Rec=0.2174, F1=0.3448\n",
            "\n",
            "-- Autoencoder A2 Results --\n",
            "Acc=0.4839, Prec=0.8889, Rec=0.3478, F1=0.5000\n",
            "\n",
            "-- Autoencoder A3 Results --\n",
            "Acc=0.3226, Prec=1.0000, Rec=0.0870, F1=0.1600\n",
            "\n",
            "=== Mutation for Transformer ===\n",
            "\n",
            "-- Transformer A1 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- Transformer A2 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- Transformer A3 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== Mutation for LSTM ===\n",
            "\n",
            "-- LSTM A1 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- LSTM A2 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- LSTM A3 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== Mutation for MLP ===\n",
            "\n",
            "-- MLP A1 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- MLP A2 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "-- MLP A3 Results --\n",
            "Acc=0.7419, Prec=0.7419, Rec=1.0000, F1=0.8519\n",
            "\n",
            "=== Mutation for DeepSVDD ===\n",
            "\n",
            "-- DeepSVDD A1 Results --\n",
            "Acc=0.7742, Prec=0.8333, Rec=0.8696, F1=0.8511\n",
            "\n",
            "-- DeepSVDD A2 Results --\n",
            "Acc=0.8065, Prec=0.9048, Rec=0.8261, F1=0.8636\n",
            "\n",
            "-- DeepSVDD A3 Results --\n",
            "Acc=0.8387, Prec=0.8462, Rec=0.9565, F1=0.8980\n",
            "\n",
            "All baseline mutation analyses completed.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Anomaly Detection on Tabular Data with 8 Models + Mutation Analysis\n",
        "\n",
        "Models:\n",
        "1. Isolation Forest\n",
        "2. One-Class SVM\n",
        "3. LocalOutlierFactor\n",
        "4. Simple Autoencoder\n",
        "5. Transformer-based model (EnergyBasedTabTransformer)\n",
        "6. LSTM-based model\n",
        "7. MLP-based model\n",
        "8. Deep SVDD\n",
        "\n",
        "Mutation Analysis: A1/A2 (large shifts => want anomaly=1)\n",
        "                    A3     (random noise => want stability)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import AlbertModel\n",
        "from torch.optim import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix)\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "############################################################################\n",
        "# 1) Global Setup & Data Loading\n",
        "############################################################################\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "file_path = 'vertebral.csv'\n",
        "df = pd.read_csv(file_path)#.drop('ID', axis=1)\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        df[col] = df[col].replace(r\"[:\\\\[\\\\],]\\'\", '', regex=True)\n",
        "    target_col = \"Class\"\n",
        "    majority_value = df[target_col].value_counts().idxmax()\n",
        "    df[target_col] = df[target_col].apply(lambda x: 1 if x == majority_value else 0)\n",
        "    df = df.fillna(df.mean())\n",
        "    return df\n",
        "\n",
        "df = clean_dataframe(df)\n",
        "\n",
        "target_col = \"Class\"\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target_col)\n",
        "all_cols     = [c for c in df.columns if c != target_col]\n",
        "categorical_cols = list(set(all_cols) - set(numeric_cols))\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "X_train = train_df[numeric_cols].values\n",
        "y_train = train_df[target_col].values\n",
        "X_test  = test_df[numeric_cols].values\n",
        "y_test  = test_df[target_col].values\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\",  test_df.shape)\n",
        "print(\"Train label distribution:\\n\", train_df[target_col].value_counts())\n",
        "print(\"Test label distribution:\\n\",  test_df[target_col].value_counts())\n",
        "\n",
        "############################################################################\n",
        "# 2) Traditional Baselines: IF, OCSVM, LOF, Autoencoder\n",
        "############################################################################\n",
        "# 2.1 Isolation Forest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iso_model = IsolationForest(n_estimators=300, random_state=SEED)\n",
        "iso_model.fit(X_train)\n",
        "iso_pred = iso_model.predict(X_test)   # +1=normal, -1=outlier\n",
        "iso_pred_label = np.where(iso_pred == -1, 1, 0)\n",
        "\n",
        "iso_acc  = accuracy_score(y_test, iso_pred_label)\n",
        "iso_prec = precision_score(y_test, iso_pred_label, zero_division=0)\n",
        "iso_rec  = recall_score(y_test, iso_pred_label, zero_division=0)\n",
        "iso_f1   = f1_score(y_test, iso_pred_label, zero_division=0)\n",
        "\n",
        "print(\"=== Isolation Forest ===\")\n",
        "print(f\"Acc={iso_acc:.4f}, Prec={iso_prec:.4f}, Rec={iso_rec:.4f}, F1={iso_f1:.4f}\")\n",
        "\n",
        "# 2.2 One-Class SVM\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "oc_svm = OneClassSVM(kernel='rbf', nu=0.1, gamma='auto')\n",
        "oc_svm.fit(X_train[y_train==0])\n",
        "svm_pred = oc_svm.predict(X_test)   # +1=normal, -1=outlier\n",
        "svm_pred_label = np.where(svm_pred==-1, 1, 0)\n",
        "\n",
        "svm_acc  = accuracy_score(y_test, svm_pred_label)\n",
        "svm_prec = precision_score(y_test, svm_pred_label, zero_division=0)\n",
        "svm_rec  = recall_score(y_test, svm_pred_label, zero_division=0)\n",
        "svm_f1   = f1_score(y_test, svm_pred_label, zero_division=0)\n",
        "\n",
        "print(\"\\n=== One-Class SVM ===\")\n",
        "print(f\"Acc={svm_acc:.4f}, Prec={svm_prec:.4f}, Rec={svm_rec:.4f}, F1={svm_f1:.4f}\")\n",
        "\n",
        "# 2.3 LocalOutlierFactor\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "lof_model = LocalOutlierFactor(n_neighbors=5, contamination=0.1, novelty=True)\n",
        "lof_model.fit(X_train)\n",
        "lof_pred = lof_model.predict(X_test)  # +1=normal, -1=outlier\n",
        "lof_pred_label = np.where(lof_pred == -1, 1, 0)\n",
        "\n",
        "lof_acc  = accuracy_score(y_test, lof_pred_label)\n",
        "lof_prec = precision_score(y_test, lof_pred_label, zero_division=0)\n",
        "lof_rec  = recall_score(y_test, lof_pred_label, zero_division=0)\n",
        "lof_f1   = f1_score(y_test, lof_pred_label, zero_division=0)\n",
        "\n",
        "print(\"\\n=== Local Outlier Factor ===\")\n",
        "print(f\"Acc={lof_acc:.4f}, Prec={lof_prec:.4f}, Rec={lof_rec:.4f}, F1={lof_f1:.4f}\")\n",
        "\n",
        "# 2.4 Simple Autoencoder\n",
        "import torch.utils.data as torchdata\n",
        "\n",
        "class SimpleAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=7, hidden_dim=4):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "tensor_train_normal = torch.tensor(X_train_normal, dtype=torch.float32)\n",
        "train_loader_ae = torchdata.DataLoader(tensor_train_normal, batch_size=32, shuffle=True)\n",
        "\n",
        "autoenc = SimpleAutoencoder(input_dim=len(numeric_cols), hidden_dim=4).to(device)\n",
        "optim_ae = torch.optim.Adam(autoenc.parameters(), lr=1e-3)\n",
        "criterion_ae = nn.MSELoss()\n",
        "\n",
        "epochs_ae = 20\n",
        "for ep in range(epochs_ae):\n",
        "    autoenc.train()\n",
        "    for batch_x in train_loader_ae:\n",
        "        batch_x = batch_x.to(device)\n",
        "        optim_ae.zero_grad()\n",
        "        recon = autoenc(batch_x)\n",
        "        loss = criterion_ae(recon, batch_x)\n",
        "        loss.backward()\n",
        "        optim_ae.step()\n",
        "\n",
        "autoenc.eval()\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "with torch.no_grad():\n",
        "    recon_test = autoenc(X_test_tensor)\n",
        "    errors = torch.mean((recon_test - X_test_tensor)**2, dim=1).cpu().numpy()\n",
        "threshold = np.percentile(errors, 90)\n",
        "ae_pred_label = (errors > threshold).astype(int)\n",
        "\n",
        "ae_acc  = accuracy_score(y_test, ae_pred_label)\n",
        "ae_prec = precision_score(y_test, ae_pred_label, zero_division=0)\n",
        "ae_rec  = recall_score(y_test, ae_pred_label, zero_division=0)\n",
        "ae_f1   = f1_score(y_test, ae_pred_label, zero_division=0)\n",
        "\n",
        "print(\"\\n=== Simple Autoencoder ===\")\n",
        "print(f\"Acc={ae_acc:.4f}, Prec={ae_prec:.4f}, Rec={ae_rec:.4f}, F1={ae_f1:.4f}\")\n",
        "\n",
        "############################################################################\n",
        "# 3) Torch-based Baselines: Transformer, LSTM, MLP, Deep SVDD\n",
        "############################################################################\n",
        "\n",
        "# Helper: numeric dataset for classification\n",
        "class NumericDataset(torchdata.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset_torch = NumericDataset(X_train, y_train)\n",
        "test_dataset_torch  = NumericDataset(X_test,  y_test)\n",
        "\n",
        "train_loader_torch = torchdata.DataLoader(train_dataset_torch, batch_size=32, shuffle=True)\n",
        "test_loader_torch  = torchdata.DataLoader(test_dataset_torch,  batch_size=32)\n",
        "\n",
        "##################### 3.1 Transformer-based Model ###########################\n",
        "from transformers import AlbertModel\n",
        "\n",
        "class EnergyBasedTabTransformer(nn.Module):\n",
        "    def __init__(self, albert_model, input_dim):\n",
        "        super().__init__()\n",
        "        self.albert_encoder = albert_model.encoder\n",
        "        self.config = albert_model.config\n",
        "        self.embedding_size = self.config.embedding_size\n",
        "\n",
        "        # We do a simple numeric -> embedding linear transform\n",
        "        self.numeric_linear = nn.Linear(input_dim, self.embedding_size)\n",
        "        # We'll skip categorical for simplicity, or we can just treat them as none\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,self.embedding_size))\n",
        "        self.energy_layer = nn.Linear(self.config.hidden_size, 1)\n",
        "        self.classifier   = nn.Linear(self.config.hidden_size, 2)\n",
        "\n",
        "    def _convert_mask(self, attention_mask, dtype):\n",
        "        extended_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(dtype=dtype)\n",
        "        return (1.0 - extended_mask) * -10000.0\n",
        "\n",
        "    def forward(self, numeric_batch):\n",
        "        \"\"\"\n",
        "        numeric_batch shape = (B, input_dim).\n",
        "        We'll reshape: numeric->(B,1,embedding_size), plus [CLS] => total length=2\n",
        "        Then pass into the ALBERT encoder (bypassing usual text).\n",
        "        \"\"\"\n",
        "        bsz = numeric_batch.size(0)\n",
        "        numeric_embed = self.numeric_linear(numeric_batch).unsqueeze(1)  # (B,1,embedding_size)\n",
        "\n",
        "        cls_tok = self.cls_token.expand(bsz, -1, -1)                     # (B,1,embedding_size)\n",
        "        seq_embeds = torch.cat([cls_tok, numeric_embed], dim=1)          # (B,2,embedding_size)\n",
        "        attention_mask = torch.ones(bsz, seq_embeds.size(1), device=seq_embeds.device)\n",
        "        extended_mask  = self._convert_mask(attention_mask, seq_embeds.dtype)\n",
        "\n",
        "        outputs = self.albert_encoder(hidden_states=seq_embeds,\n",
        "                                      attention_mask=extended_mask,\n",
        "                                      head_mask=[None]*self.config.num_hidden_layers,\n",
        "                                      output_hidden_states=True)\n",
        "        cls_embedding = outputs.hidden_states[-1][:,0,:]  # (B, hidden_size)\n",
        "\n",
        "        energy_score = self.energy_layer(cls_embedding).squeeze(-1)  # (B,)\n",
        "        logits       = self.classifier(cls_embedding)                # (B,2)\n",
        "        return energy_score, logits\n",
        "\n",
        "albert_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "transformer_model = EnergyBasedTabTransformer(albert_model, input_dim=len(numeric_cols)).to(device)\n",
        "\n",
        "##################### 3.2 LSTM-based Model ###########################\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc_classifier = nn.Linear(hidden_dim, 2)\n",
        "        self.fc_energy     = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape (B, input_dim)\n",
        "        # Treat each feature as a \"time step\" => shape => (B, input_dim, 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        lstm_out, (h,c) = self.lstm(x)\n",
        "        # lstm_out shape: (B, input_dim, hidden_dim)\n",
        "        # We'll use last time step => out = lstm_out[:,-1,:]\n",
        "        out = lstm_out[:, -1, :]\n",
        "        energy_score = self.fc_energy(out).squeeze(-1)   # (B,)\n",
        "        logits       = self.fc_classifier(out)           # (B,2)\n",
        "        return energy_score, logits\n",
        "\n",
        "lstm_model = LSTMModel(input_dim=len(numeric_cols), hidden_dim=16).to(device)\n",
        "\n",
        "##################### 3.3 MLP-based Model ###########################\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[32, 16]):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        prev = input_dim\n",
        "        for hd in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, hd))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev=hd\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(prev, 2)\n",
        "        self.energy_layer= nn.Linear(prev, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        logits = self.classifier(z)\n",
        "        energy_score = self.energy_layer(z).squeeze(-1)\n",
        "        return energy_score, logits\n",
        "\n",
        "mlp_model = MLPModel(input_dim=len(numeric_cols)).to(device)\n",
        "\n",
        "##################### 3.4 Deep SVDD-based Model ###########################\n",
        "class DeepSVDDModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[32,16], latent_dim=2):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        prev = input_dim\n",
        "        for hd in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, hd))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev=hd\n",
        "        layers.append(nn.Linear(prev, latent_dim))\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(latent_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        logits = self.classifier(latent)  # (B,2)\n",
        "        # energy => norm of latent\n",
        "        energy_score = torch.norm(latent, p=2, dim=1)\n",
        "        return energy_score, logits\n",
        "\n",
        "svdd_model = DeepSVDDModel(input_dim=len(numeric_cols), hidden_dims=[32,16], latent_dim=2).to(device)\n",
        "\n",
        "##################### Training & Evaluate Helper ###########################\n",
        "def train_deep_classifier(model, train_loader, lr=1e-3, epochs=10):\n",
        "    model.train()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for ep in range(epochs):\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            _, logits = model(Xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def predict_labels(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "        _, logits = model(X_torch)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    return preds\n",
        "\n",
        "# Train each new model & Evaluate\n",
        "for dl_model, name in [(transformer_model, \"Transformer\"),\n",
        "                       (lstm_model,        \"LSTM\"),\n",
        "                       (mlp_model,         \"MLP\"),\n",
        "                       (svdd_model,        \"DeepSVDD\")]:\n",
        "    train_deep_classifier(dl_model, train_loader_torch, lr=1e-3, epochs=10)\n",
        "    dl_pred_label = predict_labels(dl_model, X_test)\n",
        "    dl_acc  = accuracy_score(y_test, dl_pred_label)\n",
        "    dl_prec = precision_score(y_test, dl_pred_label, zero_division=0)\n",
        "    dl_rec  = recall_score(y_test, dl_pred_label, zero_division=0)\n",
        "    dl_f1   = f1_score(y_test, dl_pred_label, zero_division=0)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Acc={dl_acc:.4f}, Prec={dl_prec:.4f}, Rec={dl_rec:.4f}, F1={dl_f1:.4f}\")\n",
        "\n",
        "############################################################################\n",
        "# 4) MUTATION ANALYSIS FOR ALL MODELS\n",
        "############################################################################\n",
        "numeric_cols_for_mutation = [c for c in numeric_cols if c != \"Class\"]\n",
        "def anomaly_injection_a1_a2(\n",
        "    X,\n",
        "    mutation_fraction=0.05,\n",
        "    min_mutations=2,\n",
        "    num_features=1,\n",
        "    intensity=\"moderate\"\n",
        "):\n",
        "    intensity_levels = {\n",
        "        \"weak\":[0.25,0.5,0.75],\n",
        "        \"moderate\":[1,1.5,2],\n",
        "        \"strong\":[8,9,10]\n",
        "    }\n",
        "    if intensity not in intensity_levels:\n",
        "        raise ValueError(\"Invalid intensity for A1/A2\")\n",
        "\n",
        "    std_multipliers = intensity_levels[intensity]\n",
        "    # Use numeric_cols_for_mutation to ensure we never mutate \"Class\"\n",
        "    # or any other label column\n",
        "    if len(numeric_cols_for_mutation) < num_features:\n",
        "        raise ValueError(\"Not enough numeric columns for A1/A2\")\n",
        "\n",
        "    selected_features = random.sample(list(numeric_cols_for_mutation), num_features)\n",
        "    X = X.copy()\n",
        "    mutated_indices_set= set()\n",
        "    mutated_feature_names=[]\n",
        "\n",
        "    for feature in selected_features:\n",
        "        std_dev = X[feature].std()\n",
        "        mutation_factors = [m*std_dev for m in std_multipliers]\n",
        "        mutation_factor  = np.random.choice(mutation_factors)\n",
        "\n",
        "        num_mutations = max(min_mutations,int(mutation_fraction*len(X)))\n",
        "        num_mutations = min(num_mutations, len(X))\n",
        "\n",
        "        mutated_rows = np.random.choice(X.index, size=num_mutations, replace=False)\n",
        "        mutated_indices_set.update(mutated_rows)\n",
        "\n",
        "        before_vals = X.loc[mutated_rows, feature].copy()\n",
        "        small_value = std_dev*0.025\n",
        "        X.loc[mutated_rows, feature] = np.where(\n",
        "            X.loc[mutated_rows, feature]==0,\n",
        "            small_value/10,\n",
        "            X.loc[mutated_rows, feature]\n",
        "        )\n",
        "        X.loc[mutated_rows, feature] += mutation_factor*np.sign(X.loc[mutated_rows, feature])\n",
        "        after_vals = X.loc[mutated_rows, feature].copy()\n",
        "\n",
        "        #print(f\"A1/A2 injected in {feature} factor={mutation_factor:.3f} (intensity={intensity})\")\n",
        "        #print(pd.DataFrame({\"Before\":before_vals,\"After\":after_vals}),\"\\n\")\n",
        "\n",
        "        mutated_feature_names.append(feature)\n",
        "\n",
        "    '''if len(selected_features)>1:\n",
        "        print(f\"A2 mutated columns (together): {mutated_feature_names}\\n\")\n",
        "    else:\n",
        "        print(f\"A1 mutated column: {mutated_feature_names}\\n\")'''\n",
        "\n",
        "    mutated_indices = sorted(list(mutated_indices_set))\n",
        "    return X, mutated_indices, mutated_feature_names\n",
        "\n",
        "def anomaly_injection_a3(\n",
        "    X,\n",
        "    mutation_fraction=0.05,\n",
        "    min_mutations=2,\n",
        "    num_features=1,\n",
        "    intensity=\"moderate\"\n",
        "):\n",
        "    intensity_levels = {\n",
        "        \"weak\":0.1,\n",
        "        \"moderate\":0.5,\n",
        "        \"strong\":1.0\n",
        "    }\n",
        "    if intensity not in intensity_levels:\n",
        "        raise ValueError(\"Invalid intensity for A3\")\n",
        "\n",
        "    noise_level = intensity_levels[intensity]\n",
        "\n",
        "    # again, forcibly skip \"Class\"\n",
        "    if len(numeric_cols_for_mutation) < num_features:\n",
        "        raise ValueError(\"Not enough numeric columns for A3\")\n",
        "\n",
        "    selected_features= random.sample(numeric_cols_for_mutation, num_features)\n",
        "    X = X.copy()\n",
        "    mutated_indices_set=set()\n",
        "\n",
        "    for feature in selected_features:\n",
        "        std_dev = X[feature].std()\n",
        "        num_mutations= max(min_mutations,int(mutation_fraction*len(X)))\n",
        "        num_mutations= min(num_mutations,len(X))\n",
        "\n",
        "        mutation_rows = np.random.choice(X.index, size=num_mutations, replace=False)\n",
        "        mutated_indices_set.update(mutation_rows)\n",
        "\n",
        "        before_vals = X.loc[mutation_rows, feature].values\n",
        "        noise = np.random.normal(loc=0, scale=noise_level*std_dev, size=num_mutations)\n",
        "        X.loc[mutation_rows, feature] = np.clip(\n",
        "            X.loc[mutation_rows, feature] + noise,\n",
        "            a_min=0,\n",
        "            a_max=None\n",
        "        )\n",
        "        after_vals = X.loc[mutation_rows, feature].values\n",
        "\n",
        "        #print(f\"A3 injected in {feature}, intensity={intensity}, noise_level={noise_level}, std={std_dev:.3f}\")\n",
        "        #print(\"Before:\\n\", before_vals)\n",
        "        #print(\"After:\\n\", after_vals,\"\\n\")\n",
        "\n",
        "    mutated_indices = sorted(list(mutated_indices_set))\n",
        "    return X, mutated_indices, selected_features\n",
        "\n",
        "def predict_baseline(df_data, baseline_predict_func):\n",
        "    X_temp = df_data[numeric_cols].values\n",
        "    return baseline_predict_func(X_temp)\n",
        "\n",
        "def iso_predict_func(X):\n",
        "    p = iso_model.predict(X)   # +1=normal, -1=outlier\n",
        "    return np.where(p == -1, 1, 0)\n",
        "\n",
        "def svm_predict_func(X):\n",
        "    p = oc_svm.predict(X)      # +1=normal, -1=outlier\n",
        "    return np.where(p == -1, 1, 0)\n",
        "\n",
        "def lof_predict_func(X):\n",
        "    p = lof_model.predict(X)   # +1=normal, -1=outlier\n",
        "    return np.where(p == -1, 1, 0)\n",
        "\n",
        "def autoenc_predict_func(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        recon = autoenc(X_tensor)\n",
        "        errs = torch.mean((recon - X_tensor)**2, dim=1).cpu().numpy()\n",
        "    return (errs > threshold).astype(int)\n",
        "\n",
        "# New deep models:\n",
        "def transformer_predict_func(X):\n",
        "    return predict_labels(transformer_model, X)\n",
        "\n",
        "def lstm_predict_func(X):\n",
        "    return predict_labels(lstm_model, X)\n",
        "\n",
        "def mlp_predict_func(X):\n",
        "    return predict_labels(mlp_model, X)\n",
        "\n",
        "def svdd_predict_func(X):\n",
        "    return predict_labels(svdd_model, X)\n",
        "\n",
        "# Evaluate mutated sets\n",
        "def predict_baseline(df_data, baseline_predict_func):\n",
        "    # same approach => pass numeric columns\n",
        "    X_temp = df_data[numeric_cols].values\n",
        "    return baseline_predict_func(X_temp)\n",
        "\n",
        "def evaluate_mutation_a1_a2_baseline(df_mutated, mutated_indices, baseline_predict_func):\n",
        "    y_mut = df_mutated[target_col].values\n",
        "    pred  = predict_baseline(df_mutated, baseline_predict_func)\n",
        "    acc  = accuracy_score(y_mut, pred)\n",
        "    prec = precision_score(y_mut, pred, zero_division=0)\n",
        "    rec  = recall_score(y_mut, pred, zero_division=0)\n",
        "    f1   = f1_score(y_mut, pred, zero_division=0)\n",
        "\n",
        "    mutated_rows_pred = pred[mutated_indices]\n",
        "    predicted_anomaly = np.sum(mutated_rows_pred == 1)\n",
        "    total_mut = len(mutated_indices)\n",
        "    mutation_score = 100.0 * predicted_anomaly / total_mut if total_mut>0 else 0.0\n",
        "    return acc, prec, rec, f1, mutation_score\n",
        "\n",
        "def evaluate_mutation_a3_stability_baseline(df_mutated, mutated_indices, orig_preds, baseline_predict_func):\n",
        "    y_mut = df_mutated[target_col].values\n",
        "    new_pred = predict_baseline(df_mutated, baseline_predict_func)\n",
        "    acc  = accuracy_score(y_mut, new_pred)\n",
        "    prec = precision_score(y_mut, new_pred, zero_division=0)\n",
        "    rec  = recall_score(y_mut, new_pred, zero_division=0)\n",
        "    f1   = f1_score(y_mut, new_pred, zero_division=0)\n",
        "\n",
        "    changed = 0\n",
        "    for row_idx in mutated_indices:\n",
        "        if orig_preds[row_idx] != new_pred[row_idx]:\n",
        "            changed +=1\n",
        "    total = len(mutated_indices)\n",
        "    stability_score = 100.0*(total-changed)/total if total>0 else 100.0\n",
        "    return acc, prec, rec, f1, stability_score\n",
        "\n",
        "def run_mutation_for_baseline(baseline_name, baseline_predict_func, orig_preds):\n",
        "    print(f\"\\n=== Mutation for {baseline_name} ===\")\n",
        "    # A1\n",
        "    df_a1, mut_idxs_a1, _ = anomaly_injection_a1_a2(\n",
        "        X=test_df.copy(), mutation_fraction=0.1, min_mutations=2,\n",
        "        num_features=1, intensity=\"strong\"\n",
        "    )\n",
        "    a1_acc,a1_prec,a1_rec,a1_f1,a1_score = evaluate_mutation_a1_a2_baseline(df_a1, mut_idxs_a1, baseline_predict_func)\n",
        "    print(f\"\\n-- {baseline_name} A1 Results --\")\n",
        "    print(f\"Acc={a1_acc:.4f}, Prec={a1_prec:.4f}, Rec={a1_rec:.4f}, F1={a1_f1:.4f}\")\n",
        "    #print(f\"Mutation Score (A1)={a1_score:.1f}%\")\n",
        "\n",
        "    # A2\n",
        "    df_a2, mut_idxs_a2, _ = anomaly_injection_a1_a2(\n",
        "        X=test_df.copy(), mutation_fraction=0.1, min_mutations=2,\n",
        "        num_features=3, intensity=\"strong\"\n",
        "    )\n",
        "    a2_acc,a2_prec,a2_rec,a2_f1,a2_score = evaluate_mutation_a1_a2_baseline(df_a2, mut_idxs_a2, baseline_predict_func)\n",
        "    print(f\"\\n-- {baseline_name} A2 Results --\")\n",
        "    print(f\"Acc={a2_acc:.4f}, Prec={a2_prec:.4f}, Rec={a2_rec:.4f}, F1={a2_f1:.4f}\")\n",
        "    #print(f\"Mutation Score (A2)={a2_score:.1f}%\")\n",
        "\n",
        "    # A3\n",
        "    df_a3, mut_idxs_a3, _ = anomaly_injection_a3(\n",
        "        X=test_df.copy(), mutation_fraction=0.1, min_mutations=2,\n",
        "        num_features=2, intensity=\"strong\"\n",
        "    )\n",
        "    a3_acc,a3_prec,a3_rec,a3_f1,a3_stability = evaluate_mutation_a3_stability_baseline(\n",
        "        df_a3, mut_idxs_a3, orig_preds, baseline_predict_func\n",
        "    )\n",
        "    print(f\"\\n-- {baseline_name} A3 Results --\")\n",
        "    print(f\"Acc={a3_acc:.4f}, Prec={a3_prec:.4f}, Rec={a3_rec:.4f}, F1={a3_f1:.4f}\")\n",
        "    #print(f\"Stability Score (A3)={a3_stability:.1f}%\")\n",
        "\n",
        "############################################################################\n",
        "# 5) Final: Mutation Analysis for All 8 Models\n",
        "############################################################################\n",
        "print(\"\\n========== BASELINE MUTATION ANALYSIS ==========\")\n",
        "# 5.1 Traditional scikit-learn:\n",
        "iso_orig_pred = iso_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"IsolationForest\", iso_predict_func, iso_orig_pred)\n",
        "\n",
        "svm_orig_pred = svm_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"OneClassSVM\", svm_predict_func, svm_orig_pred)\n",
        "\n",
        "lof_orig_pred = lof_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"LocalOutlierFactor\", lof_predict_func, lof_orig_pred)\n",
        "\n",
        "ae_orig_pred = autoenc_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"Autoencoder\", autoenc_predict_func, ae_orig_pred)\n",
        "\n",
        "# 5.2 Deep Models:\n",
        "transformer_orig_pred = transformer_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"Transformer\", transformer_predict_func, transformer_orig_pred)\n",
        "\n",
        "lstm_orig_pred = lstm_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"LSTM\", lstm_predict_func, lstm_orig_pred)\n",
        "\n",
        "mlp_orig_pred = mlp_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"MLP\", mlp_predict_func, mlp_orig_pred)\n",
        "\n",
        "svdd_orig_pred = svdd_predict_func(X_test)\n",
        "run_mutation_for_baseline(\"DeepSVDD\", svdd_predict_func, svdd_orig_pred)\n",
        "\n",
        "print(\"\\nAll baseline mutation analyses completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}