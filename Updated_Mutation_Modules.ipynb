{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrYMC0B2B0YS"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# Updated Mutatation Modules\n",
        "# =========================================\n",
        "\n",
        "def mutate_test_set(\n",
        "    X_test: np.ndarray,\n",
        "    partitions=None,\n",
        "    mutation_rate: float = 0.10,\n",
        "    seed: int = 0,\n",
        "    a4_noise_std: float = 0.02,\n",
        "    ensure_all_types: bool = True,\n",
        "    *,\n",
        "    data_type: str = \"tabular\",     # {\"tabular\",\"spatial\",\"temporal\"}\n",
        "    k_ctx: int = 5,                 # used for spatial A3 (kNN)\n",
        "    n_clusters: int | None = None,  # used for tabular A3 (KMeans); default ~sqrt(N)\n",
        "    window: int | None = None,      # used for temporal A3; default ~10% of N\n",
        "    extreme_mult: float = 0.75      # how far beyond observed range to push A1/A2\n",
        ") -> tuple[np.ndarray, dict[int, object]]:\n",
        "    \"\"\"\n",
        "    Inject **synthetic anomalies** into X_test for mutation-based evaluation.\n",
        "\n",
        "    We create a mutated copy `X_mut` and a metadata map `metas` that records the ground-truth\n",
        "    *explanation target* for each mutated row:\n",
        "\n",
        "        • **A1 (feature-level corruption)**: pick one feature f and push its value **beyond** the\n",
        "          observed range (to hi + m·span or lo − m·span).  Meta → (f, f).\n",
        "        • **A2 (cross-feature conflict)**: pick two features f1≠f2 and push them to **opposite**\n",
        "          extremes beyond the range to maximize inconsistency.  Meta → (f1, f2).\n",
        "        • **A3 (contextual inconsistency)**:\n",
        "              – data_type=\"tabular\": KMeans clusters; replace row i with a donor from the\n",
        "                **farthest cluster** (cluster whose centroid is farthest from i), maximizing\n",
        "                context shift.\n",
        "              – data_type=\"spatial\": kNN; replace with the **farthest** point (top 5% distances),\n",
        "                not in the nearest-neighbor set.\n",
        "              – data_type=\"temporal\": windowing; replace with a row **window** steps away\n",
        "                (default ≈10% of N), i.e., donor = (i + window) mod N.\n",
        "            Meta → \"context\".\n",
        "        • **A4 (benign noise)**: add small Gaussian noise per feature (no anomaly expected).\n",
        "          Meta → None.\n",
        "\n",
        "    The function balances A1–A4 across the selected rows (when feasible) and never samples more\n",
        "    rows than available.  For small N/D we fall back gracefully (e.g., A2→A1 if D<2).\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    X_test : (N, D) ndarray\n",
        "        Test matrix to mutate (rows are records).\n",
        "    partitions : ignored (kept for API compatibility).\n",
        "    mutation_rate : float\n",
        "        Fraction of rows to mutate (capped by N). If `ensure_all_types` and N≥4, we ensure each\n",
        "        type appears at least once.\n",
        "    seed : int\n",
        "        RNG seed.\n",
        "    a4_noise_std : float\n",
        "        Std multiplier for A4 benign noise (small so A4 remains mostly non-anomalous).\n",
        "    ensure_all_types : bool\n",
        "        If True and N≥4, distribute mutations across A1..A4.\n",
        "    data_type : {\"tabular\",\"spatial\",\"temporal\"}\n",
        "        Strategy for **A3** (context) mutations.\n",
        "    k_ctx : int\n",
        "        #neighbors for spatial kNN.\n",
        "    n_clusters : int or None\n",
        "        #clusters for KMeans in tabular A3. Default ≈ sqrt(N).\n",
        "    window : int or None\n",
        "        Temporal offset for A3 windowing. Default ≈ 10% of N (at least 5).\n",
        "    extreme_mult : float\n",
        "        How far beyond the observed [lo, hi] we push A1/A2. New value = hi + m·(hi−lo) or\n",
        "        lo − m·(hi−lo). Use 0.5–1.0 to make anomalies stronger.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_mut : (N, D) ndarray\n",
        "        Mutated copy of X_test.\n",
        "    metas : dict[int, object]\n",
        "        Mapping row index → expected explanation label:\n",
        "          A1→(f,f), A2→(f1,f2), A3→\"context\", A4→None.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X_mut = np.array(X_test, copy=True)\n",
        "    N, D  = X_mut.shape\n",
        "\n",
        "    if N == 0:\n",
        "        return X_mut, {}\n",
        "\n",
        "    # ----- how many rows to mutate (cap by N) -----\n",
        "    requested = int(round(mutation_rate * N))\n",
        "    if ensure_all_types and N >= 4:\n",
        "        requested = max(requested, 4)     # ensure each type appears\n",
        "    requested = max(1, min(requested, N)) # never exceed N\n",
        "\n",
        "    # sample indices WITHOUT replacement\n",
        "    sel = rng.choice(N, size=requested, replace=False)\n",
        "\n",
        "    # ----- split selected rows across A1..A4 (balanced, with remainder) -----\n",
        "    if ensure_all_types and N >= 4:\n",
        "        base, rem = divmod(requested, 4)\n",
        "        counts = [base + (i < rem) for i in range(4)]\n",
        "        type_order = ([\"A1\"] * counts[0] +\n",
        "                      [\"A2\"] * counts[1] +\n",
        "                      [\"A3\"] * counts[2] +\n",
        "                      [\"A4\"] * counts[3])\n",
        "    else:\n",
        "        all_types = [\"A1\", \"A2\", \"A3\", \"A4\"]\n",
        "        type_order = [all_types[i % 4] for i in range(requested)]\n",
        "\n",
        "    # ---- helpers ----\n",
        "    def _lo_hi_span(col: np.ndarray) -> tuple[float, float, float]:\n",
        "        lo, hi = np.nanpercentile(col, [0.1, 99.9])  # tighter tails → stronger extremes\n",
        "        if not np.isfinite(lo) or not np.isfinite(hi):\n",
        "            lo, hi = float(np.nanmin(col)), float(np.nanmax(col))\n",
        "        if lo == hi:\n",
        "            hi = lo + 1e-6\n",
        "        return lo, hi, (hi - lo)\n",
        "\n",
        "    metas: dict[int, object] = {}\n",
        "\n",
        "    # ---------- Precompute structures for A3 ----------\n",
        "    mode = (data_type or \"tabular\").lower()\n",
        "    if mode == \"static\":\n",
        "        mode = \"tabular\"\n",
        "\n",
        "    # TABULAR (KMeans across rows → far cluster donor)\n",
        "    if mode == \"tabular\" and N >= 2:\n",
        "        try:\n",
        "            from sklearn.cluster import KMeans\n",
        "            C = n_clusters if n_clusters is not None else int(max(2, min(N, round(np.sqrt(N)))))\n",
        "            try:\n",
        "                km = KMeans(n_clusters=C, random_state=seed, n_init=\"auto\")\n",
        "            except TypeError:\n",
        "                km = KMeans(n_clusters=C, random_state=seed, n_init=10)\n",
        "            labels = km.fit_predict(X_test)\n",
        "            centroids = km.cluster_centers_   # (C,D)\n",
        "            # members per cluster\n",
        "            from collections import defaultdict\n",
        "            cluster_members = defaultdict(list)\n",
        "            for i, lab in enumerate(labels):\n",
        "                cluster_members[int(lab)].append(i)\n",
        "        except Exception:\n",
        "            labels, centroids, cluster_members = None, None, None\n",
        "\n",
        "    # SPATIAL (distance matrix / farthest donor)\n",
        "    if mode == \"spatial\" and N >= 2:\n",
        "        # full pairwise Euclidean distances (ok for typical N; for very large N replace with ANN)\n",
        "        diffs = X_test[:, None, :] - X_test[None, :, :]\n",
        "        distM = np.sqrt(np.sum(diffs * diffs, axis=2))  # (N,N)\n",
        "\n",
        "    # TEMPORAL (window offset)\n",
        "    if mode == \"temporal\":\n",
        "        if window is None:\n",
        "            window = max(5, int(round(0.10 * N)))  # ~10% of N\n",
        "        window = int(max(1, min(window, max(1, N-1))))\n",
        "\n",
        "    # ---------- apply mutations ----------\n",
        "    for idx, mtype in zip(sel, type_order):\n",
        "        idx = int(idx)\n",
        "\n",
        "        if mtype == \"A1\":\n",
        "            # push ONE feature beyond observed range by extreme_mult * span\n",
        "            f = int(rng.integers(0, max(1, D)))\n",
        "            lo, hi, span = _lo_hi_span(X_test[:, f])\n",
        "            med = np.nanmedian(X_test[:, f])\n",
        "            if X_test[idx, f] < med:\n",
        "                new_val = hi + extreme_mult * span\n",
        "            else:\n",
        "                new_val = lo - extreme_mult * span\n",
        "            jitter = float(rng.normal(0.0, 0.02 * span))\n",
        "            X_mut[idx, f] = float(new_val + jitter)\n",
        "            metas[idx] = (f, f)\n",
        "\n",
        "        elif mtype == \"A2\":\n",
        "            # pick two features; prefer different partitions if provided\n",
        "            if D >= 2:\n",
        "                if partitions and len(partitions) >= 2 and sum(len(p) for p in partitions) == D:\n",
        "                    # sample two distinct partitions then a feature from each\n",
        "                    p1, p2 = rng.choice(len(partitions), size=2, replace=False)\n",
        "                    f1 = int(rng.choice(partitions[p1]))\n",
        "                    f2 = int(rng.choice(partitions[p2]))\n",
        "                else:\n",
        "                    f1, f2 = rng.choice(D, size=2, replace=False)\n",
        "                lo1, hi1, s1 = _lo_hi_span(X_test[:, f1])\n",
        "                lo2, hi2, s2 = _lo_hi_span(X_test[:, f2])\n",
        "                # push to opposite sides, beyond range\n",
        "                X_mut[idx, f1] = hi1 + extreme_mult * s1\n",
        "                X_mut[idx, f2] = lo2 - extreme_mult * s2\n",
        "                metas[idx] = (int(f1), int(f2))\n",
        "            else:\n",
        "                # fallback to A1\n",
        "                f = 0\n",
        "                lo, hi, span = _lo_hi_span(X_test[:, f])\n",
        "                X_mut[idx, f] = hi + extreme_mult * span\n",
        "                metas[idx] = (f, f)\n",
        "\n",
        "        elif mtype == \"A3\":\n",
        "            if N < 2:\n",
        "                # fallback to A4\n",
        "                noise = rng.normal(0.0, a4_noise_std, size=D).astype(float)\n",
        "                X_mut[idx] = X_mut[idx] + noise\n",
        "                metas[idx] = None\n",
        "                continue\n",
        "\n",
        "            if mode == \"tabular\" and labels is not None and centroids is not None:\n",
        "                lab_i = int(labels[idx])\n",
        "                # pick the farthest centroid from x_i\n",
        "                d2c = np.linalg.norm(centroids - X_test[idx], axis=1)\n",
        "                far_lab = int(np.argmax(d2c))\n",
        "                # choose donor from far_lab with largest distance to x_i\n",
        "                cand = cluster_members[far_lab]\n",
        "                if len(cand) == 0:\n",
        "                    cand = [j for j in range(N) if j != idx]\n",
        "                dists = np.linalg.norm(X_test[cand] - X_test[idx], axis=1)\n",
        "                donor = int(cand[int(np.argmax(dists))])\n",
        "\n",
        "            elif mode == \"spatial\":\n",
        "                # farthest (top 5%) non-neighbor\n",
        "                drow = distM[idx].copy()\n",
        "                drow[idx] = -np.inf\n",
        "                # pick from top-5% farthest\n",
        "                k = max(1, int(round(0.05 * N)))\n",
        "                farset = np.argsort(drow)[-k:]\n",
        "                donor = int(rng.choice(farset))\n",
        "\n",
        "            elif mode == \"temporal\":\n",
        "                donor = int((idx + window) % N)\n",
        "\n",
        "            else:\n",
        "                # generic fallback: farthest by Euclidean distance\n",
        "                drow = np.linalg.norm(X_test - X_test[idx], axis=1)\n",
        "                drow[idx] = -np.inf\n",
        "                donor = int(np.argmax(drow))\n",
        "\n",
        "            X_mut[idx] = X_test[donor]\n",
        "            metas[idx] = \"context\"\n",
        "\n",
        "        elif mtype == \"A4\":\n",
        "            # small benign noise (kept small so A4 remains mostly non-anomalous)\n",
        "            noise = rng.normal(0.0, a4_noise_std, size=D).astype(float)\n",
        "            X_mut[idx] = X_mut[idx] + noise\n",
        "            metas[idx] = None\n",
        "\n",
        "        else:\n",
        "            # safety fallback → benign noise\n",
        "            noise = rng.normal(0.0, a4_noise_std, size=D).astype(float)\n",
        "            X_mut[idx] = X_mut[idx] + noise\n",
        "            metas[idx] = None\n",
        "\n",
        "    return X_mut, metas\n",
        "\n",
        "\n",
        "\n",
        "# ==================== Evaluate Explanations on Mutations ====================\n",
        "def evaluate_explanations_on_mutations(\n",
        "    explanations: Dict[int, Dict],\n",
        "    metas: Dict[int, Optional[Tuple[int,int]]],\n",
        "    partitions: List[List[int]],\n",
        "    topM: int = 3,                 # how many top disagreement pairs to consider\n",
        "    use_violated_deps: bool = True,# also accept any pair in ex[\"violated_dependencies\"]\n",
        "    y_true: Optional[np.ndarray] = None,  # optional: if provided, p90 is computed on clean rows\n",
        "    ref_max_dis_clean: Optional[np.ndarray] = None,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Mutation-based explanation evaluation (A1–A4), improved:\n",
        "\n",
        "    • A1 (feature corruption): success if ANY of the top-M disagreement pairs\n",
        "      includes the mutated partition k, OR if any violated dependency involves k.\n",
        "    • A2 (cross-feature inconsistency): success if the unordered expected pair {k1,k2}\n",
        "      is in the top-M disagreement pairs OR in violated dependencies.\n",
        "    • A3 (context): success if ex['context_flag'] is True (or similarity < 0.9 fallback).\n",
        "    • A4 (noise): success if no violated deps AND max_disagreement <= p90 of clean refs.\n",
        "\n",
        "    Returns per-type accuracy and overall.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    def _top_pairs_from_D(D: np.ndarray, M: int = 3) -> set:\n",
        "        \"\"\"Return set of up to M pairs (k,l) with largest D (k<l).\"\"\"\n",
        "        if D is None:\n",
        "            return set()\n",
        "        K = D.shape[0]\n",
        "        tri = np.triu(D, 1)\n",
        "        # Handle pathological all-NaN/const\n",
        "        if not np.isfinite(tri).any():\n",
        "            return set()\n",
        "        order = np.argsort(tri, axis=None)[::-1]  # descending by disagreement\n",
        "        pairs = set()\n",
        "        for idx in order:\n",
        "            k, l = np.unravel_index(idx, (K, K))\n",
        "            if k < l:\n",
        "                pairs.add((int(k), int(l)))\n",
        "                if len(pairs) >= M:\n",
        "                    break\n",
        "        return pairs\n",
        "\n",
        "    f2p = _feature_to_partition_map(partitions)\n",
        "    total = {\"A1\":0, \"A2\":0, \"A3\":0, \"A4\":0}\n",
        "    correct = {\"A1\":0, \"A2\":0, \"A3\":0, \"A4\":0}\n",
        "\n",
        "    # --- Reference p90 for A4 (prefer clean rows if y_true provided) ---\n",
        "    if y_true is not None:\n",
        "        clean_idxs = [i for i in explanations.keys() if y_true[i] == 0]\n",
        "        ref_max_dis = [explanations[i][\"max_disagreement\"] for i in clean_idxs]\n",
        "    else:\n",
        "        # fallback: approximate clean as those with no dep violations & no context flag\n",
        "        ref_max_dis = [\n",
        "            ex[\"max_disagreement\"]\n",
        "            for ex in explanations.values()\n",
        "            if not ex.get(\"violated_dependencies\", []) and not ex.get(\"context_flag\", False)\n",
        "        ]\n",
        "\n",
        "\n",
        "    # --- Reference p90 for A4, prefer truly clean distribution ---\n",
        "    if ref_max_dis_clean is not None and len(ref_max_dis_clean) >= 20:\n",
        "        p90 = float(np.quantile(ref_max_dis_clean, 0.90))\n",
        "    elif y_true is not None:\n",
        "        clean_idxs = [i for i in explanations.keys() if y_true[i] == 0]\n",
        "        ref_max_dis = [explanations[i][\"max_disagreement\"] for i in clean_idxs]\n",
        "        p90 = float(np.quantile(ref_max_dis, 0.90)) if len(ref_max_dis) >= 20 else 0.2\n",
        "    else:\n",
        "        ref_max_dis = [\n",
        "            ex[\"max_disagreement\"]\n",
        "            for ex in explanations.values()\n",
        "            if not ex.get(\"violated_dependencies\", []) and not ex.get(\"context_flag\", False)\n",
        "        ]\n",
        "        p90 = float(np.quantile(ref_max_dis, 0.90)) if len(ref_max_dis) >= 20 else 0.2\n",
        "\n",
        "    for idx, expected in metas.items():\n",
        "        ex = explanations.get(idx)\n",
        "        if ex is None:\n",
        "            continue\n",
        "\n",
        "        # Build candidate pairs: top-M by D plus (optionally) violated deps\n",
        "        D = ex.get(\"disagreement_matrix\", None)\n",
        "        cand_pairs = _top_pairs_from_D(D, M=topM)\n",
        "        if use_violated_deps:\n",
        "            viol = [tuple(sorted(p)) for p in ex.get(\"violated_dependencies\", [])]\n",
        "            cand_pairs |= set(viol)\n",
        "\n",
        "        if expected == \"context\":\n",
        "            total[\"A3\"] += 1\n",
        "            ok = bool(ex.get(\"context_flag\", False))\n",
        "            if not ok and ex.get(\"context_similarity\") is not None:\n",
        "                ok = float(ex[\"context_similarity\"]) < 0.9\n",
        "            correct[\"A3\"] += int(ok)\n",
        "\n",
        "        elif expected is None:\n",
        "            total[\"A4\"] += 1\n",
        "            ok = (len(ex.get(\"violated_dependencies\", [])) == 0) and (ex[\"max_disagreement\"] <= p90)\n",
        "            correct[\"A4\"] += int(ok)\n",
        "\n",
        "        else:\n",
        "            # expected is (f1, f2)\n",
        "            f1, f2 = expected\n",
        "            k1, k2 = f2p[int(f1)], f2p[int(f2)]\n",
        "            exp_pair = tuple(sorted((k1, k2)))\n",
        "\n",
        "            if k1 == k2:\n",
        "                # A1: success if any candidate pair mentions k1\n",
        "                total[\"A1\"] += 1\n",
        "                ok = any((k1 == a or k1 == b) for (a, b) in cand_pairs)\n",
        "                correct[\"A1\"] += int(ok)\n",
        "            else:\n",
        "                # A2: success if expected pair is among candidates\n",
        "                total[\"A2\"] += 1\n",
        "                ok = (exp_pair in cand_pairs)\n",
        "                correct[\"A2\"] += int(ok)\n",
        "\n",
        "    # Accuracies\n",
        "    accs = {}\n",
        "    for t in [\"A1\",\"A2\",\"A3\",\"A4\"]:\n",
        "        accs[t] = (correct[t] / total[t]) if total[t] > 0 else np.nan\n",
        "    tot = sum(total.values())\n",
        "    cor = sum(correct.values())\n",
        "    accs[\"overall\"] = (cor / tot) if tot > 0 else np.nan\n",
        "    return accs"
      ]
    }
  ]
}