{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Energy-Based Anomaly Detection from Tabular Data using ALBERT\n",
        "\n",
        "Heads:\n",
        "  - energy head (scalar energy) with hinge margins\n",
        "  - classifier head (2 classes) with cross-entropy\n",
        "\n",
        "We use two-phase training:\n",
        "  Phase 1: classification only\n",
        "  Phase 2: turn on margin losses\n",
        "  + a Triplet loss for \"shifted normal.\"\n",
        "\n",
        "Then we do Mutation Analysis:\n",
        "  - A1/A2 (large shifts => want the mutated rows to become anomalies)\n",
        "  - A3 (random noise => want predictions to remain unchanged)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AlbertModel\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, roc_auc_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1) Reproducibility & Device\n",
        "# ---------------------------------------------------------------------------\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\"+str(device))\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2) Load & Preprocess\n",
        "# ---------------------------------------------------------------------------\n",
        "file_path = 'creditcard.csv'\n",
        "df = pd.read_csv(file_path).drop([\"Time\"], axis=1)\n",
        "target_col = \"Class\"\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        df[col] = df[col].replace(r\"[:\\\\[\\\\],]\\'\", '', regex=True)\n",
        "    majority_value = df[target_col].value_counts().idxmax()\n",
        "    df[target_col] = df[target_col].apply(lambda x: 0 if x == majority_value else 1)\n",
        "    df = df.fillna(df.mean())\n",
        "    return df\n",
        "\n",
        "df = clean_dataframe(df)\n",
        "\n",
        "print(df[target_col].value_counts())\n",
        "\n",
        "all_cols = [c for c in df.columns if c != target_col]\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target_col)\n",
        "categorical_cols = list(set(all_cols) - set(numeric_cols))\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "cat_id_maps = {}\n",
        "cat_dims = {}\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype('category')  # ensure consistent category\n",
        "    categories = df[col].cat.categories\n",
        "    cat_id_maps[col] = {cat: i for i, cat in enumerate(categories)}\n",
        "    cat_dims[col] = len(categories)\n",
        "\n",
        "def encode_categorical(df, col):\n",
        "    return df[col].apply(lambda x: cat_id_maps[col].get(x, 0)).values\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3) Dataset & Dataloader\n",
        "# ---------------------------------------------------------------------------\n",
        "class AnomalyTabularDataset(Dataset):\n",
        "    def __init__(self, data_df, numeric_cols, categorical_cols, target_col):\n",
        "        self.df = data_df\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.categorical_cols = categorical_cols\n",
        "\n",
        "        self.numeric_data = self.df[self.numeric_cols].astype(np.float32).values\n",
        "\n",
        "        cat_data_list = []\n",
        "        for c in self.categorical_cols:\n",
        "            cat_data_list.append(encode_categorical(self.df, c))\n",
        "        if len(cat_data_list) == 0:\n",
        "            self.cat_data = np.zeros((len(self.df), 0), dtype=np.int64)\n",
        "        else:\n",
        "            self.cat_data = np.stack(cat_data_list, axis=1).astype(np.int64)\n",
        "\n",
        "        self.labels = torch.tensor(self.df[target_col].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        numeric_row = self.numeric_data[idx]\n",
        "        cat_row = self.cat_data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return {\n",
        "            \"numeric\": numeric_row,\n",
        "            \"categorical\": cat_row,\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "def tabular_collate_fn(batch):\n",
        "    numeric_list = []\n",
        "    cat_list     = []\n",
        "    labels_list  = []\n",
        "    for item in batch:\n",
        "        numeric_list.append(item[\"numeric\"])\n",
        "        cat_list.append(item[\"categorical\"])\n",
        "        labels_list.append(item[\"label\"])\n",
        "    numeric_tensor = torch.tensor(np.stack(numeric_list), dtype=torch.float32)\n",
        "    cat_tensor     = torch.tensor(np.stack(cat_list), dtype=torch.long)\n",
        "    labels_tensor  = torch.stack(labels_list)\n",
        "    return {\n",
        "        \"numeric\": numeric_tensor,\n",
        "        \"categorical\": cat_tensor,\n",
        "        \"labels\": labels_tensor\n",
        "    }\n",
        "\n",
        "train_dataset = AnomalyTabularDataset(train_df, numeric_cols, categorical_cols, target_col)\n",
        "test_dataset  = AnomalyTabularDataset(test_df,  numeric_cols, categorical_cols, target_col)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=tabular_collate_fn)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=64, collate_fn=tabular_collate_fn)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4) EnergyBasedTabTransformer\n",
        "# ---------------------------------------------------------------------------\n",
        "class EnergyBasedTabTransformer(nn.Module):\n",
        "    def __init__(self, albert_model, numeric_dim, categorical_cols, cat_dims):\n",
        "        super().__init__()\n",
        "        self.albert_encoder = albert_model.encoder\n",
        "        self.config = albert_model.config\n",
        "        self.embedding_size = self.config.embedding_size\n",
        "\n",
        "        self.numeric_linear = nn.Linear(numeric_dim, self.embedding_size)\n",
        "\n",
        "        self.categorical_cols = categorical_cols\n",
        "        self.cat_embeddings = nn.ModuleList()\n",
        "        for col in categorical_cols:\n",
        "            vocab_size = cat_dims[col]\n",
        "            emb = nn.Embedding(vocab_size, self.embedding_size)\n",
        "            self.cat_embeddings.append(emb)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,self.embedding_size))\n",
        "        self.energy_layer = nn.Linear(self.config.hidden_size, 1)\n",
        "        self.classifier   = nn.Linear(self.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, numeric, categorical, return_cls_emb=False):\n",
        "        bsz = numeric.size(0)\n",
        "        numeric_embed = self.numeric_linear(numeric).unsqueeze(1)\n",
        "\n",
        "        if len(self.categorical_cols) > 0:\n",
        "            cat_embeds_list = []\n",
        "            for i, emb_layer in enumerate(self.cat_embeddings):\n",
        "                cat_ids = categorical[:, i]\n",
        "                cat_emb = emb_layer(cat_ids)\n",
        "                cat_embeds_list.append(cat_emb.unsqueeze(1))\n",
        "            cat_embeds = torch.cat(cat_embeds_list, dim=1)\n",
        "        else:\n",
        "            cat_embeds = torch.zeros(bsz, 0, self.embedding_size, device=numeric.device)\n",
        "\n",
        "        cls_tok = self.cls_token.expand(bsz, -1, -1)\n",
        "        seq_embeds = torch.cat([cls_tok, numeric_embed, cat_embeds], dim=1)\n",
        "        seq_len = seq_embeds.size(1)\n",
        "\n",
        "        attention_mask = torch.ones(bsz, seq_len, device=seq_embeds.device)\n",
        "        extended_mask  = self._convert_mask(attention_mask, seq_embeds.dtype)\n",
        "\n",
        "        outputs = self.albert_encoder(\n",
        "            hidden_states=seq_embeds,\n",
        "            attention_mask=extended_mask,\n",
        "            head_mask=[None]*self.config.num_hidden_layers,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "        cls_embedding = last_hidden_state[:,0,:]\n",
        "\n",
        "        energy_score = self.energy_layer(cls_embedding).squeeze(-1)\n",
        "        logits       = self.classifier(cls_embedding)\n",
        "\n",
        "        if return_cls_emb:\n",
        "            return energy_score, logits, cls_embedding\n",
        "        else:\n",
        "            return energy_score, logits\n",
        "\n",
        "    def _convert_mask(self, attention_mask, dtype):\n",
        "        extended_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(dtype=dtype)\n",
        "        extended_mask = (1.0 - extended_mask)*(-10000.0)\n",
        "        return extended_mask\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5) Combined Loss: CE + margin\n",
        "# ---------------------------------------------------------------------------\n",
        "def combined_loss(energy_scores, logits, labels,\n",
        "                  margin_normal, margin_anomaly,\n",
        "                  lambda_cls=1.0,\n",
        "                  lambda_margin_normal=1.0,\n",
        "                  lambda_margin_anomaly=1.0,\n",
        "                  weight=None,\n",
        "                  return_components=False):\n",
        "    \"\"\"\n",
        "    L_total = λ_cls * L_cls + (λ_margin_normal * L_cov + λ_margin_anomaly * L_anom)\n",
        "    If return_components=True, also return (L_cls.item(), L_margin.item()) for logging.\n",
        "    \"\"\"\n",
        "    #weight=None\n",
        "    if weight is not None:\n",
        "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    L_cls = criterion(logits, labels)\n",
        "\n",
        "    normal_mask  = (labels == 0)\n",
        "    anomaly_mask = (labels == 1)\n",
        "\n",
        "    L_cov = torch.mean(F.relu(energy_scores[normal_mask] - margin_normal)) if normal_mask.sum()>0 else 0.0\n",
        "    L_anom= torch.mean(F.relu(margin_anomaly - energy_scores[anomaly_mask])) if anomaly_mask.sum()>0 else 0.0\n",
        "    L_margin = lambda_margin_normal * L_cov + lambda_margin_anomaly * L_anom\n",
        "\n",
        "    total_loss = lambda_cls * L_cls + L_margin\n",
        "\n",
        "    if return_components:\n",
        "        return total_loss, L_cls.item(), L_margin.item()\n",
        "    else:\n",
        "        return total_loss\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6) Evaluate Model\n",
        "# ---------------------------------------------------------------------------\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            numeric = batch[\"numeric\"].to(device)\n",
        "            categorical = batch[\"categorical\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            _, logits = model(numeric, categorical)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_logits.append(logits.cpu().numpy())\n",
        "\n",
        "    all_logits = np.concatenate(all_logits, axis=0)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds  = np.array(all_preds)\n",
        "\n",
        "    accuracy  = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall    = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1        = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    return accuracy, precision, recall, f1, all_labels, all_preds, all_logits\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 7) Visualization in 2D\n",
        "# ---------------------------------------------------------------------------\n",
        "def visualize_in_2D(model, data_loader, epoch=0,\n",
        "                    margin_normal=-0.1, margin_anomaly=0.1):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_energies   = []\n",
        "    all_labels     = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            numeric = batch[\"numeric\"].to(device)\n",
        "            categorical = batch[\"categorical\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            energy_scores, _, cls_emb = model(numeric, categorical, return_cls_emb=True)\n",
        "\n",
        "            all_embeddings.append(cls_emb.cpu().numpy())\n",
        "            all_energies.append(energy_scores.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    embeddings_array = np.concatenate(all_embeddings, axis=0)\n",
        "    energies_array   = np.concatenate(all_energies, axis=0)\n",
        "    labels_array     = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    coords_2D = pca.fit_transform(embeddings_array)\n",
        "\n",
        "    normal_mask  = (labels_array==0)\n",
        "    anomaly_mask = (labels_array==1)\n",
        "\n",
        "    zoneA_mask = (energies_array < margin_normal)\n",
        "    zoneB_mask = (energies_array>=margin_normal)&(energies_array<=margin_anomaly)\n",
        "    zoneC_mask = (energies_array>margin_anomaly)\n",
        "\n",
        "    normalA = normal_mask & zoneA_mask\n",
        "    normalB = normal_mask & zoneB_mask\n",
        "    normalC = normal_mask & zoneC_mask\n",
        "    anomA   = anomaly_mask & zoneA_mask\n",
        "    anomB   = anomaly_mask & zoneB_mask\n",
        "    anomC   = anomaly_mask & zoneC_mask\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(f\"2D Visualization - Epoch {epoch}\")\n",
        "\n",
        "    # Normal\n",
        "    plt.scatter(coords_2D[normalA,0], coords_2D[normalA,1], marker='.', label=f\"Normal, E<{margin_normal}\")\n",
        "    plt.scatter(coords_2D[normalB,0], coords_2D[normalB,1], marker='o', label=f\"Normal, {margin_normal}<=E<={margin_anomaly}\")\n",
        "    plt.scatter(coords_2D[normalC,0], coords_2D[normalC,1], marker='x', label=f\"Normal, E>{margin_anomaly}\")\n",
        "\n",
        "    # Anomaly\n",
        "    plt.scatter(coords_2D[anomA,0], coords_2D[anomA,1], marker='^', label=f\"Anomaly, E<{margin_normal}\")\n",
        "    plt.scatter(coords_2D[anomB,0], coords_2D[anomB,1], marker='s', label=f\"Anomaly, {margin_normal}<=E<={margin_anomaly}\")\n",
        "    plt.scatter(coords_2D[anomC,0], coords_2D[anomC,1], marker='D', label=f\"Anomaly, E>{margin_anomaly}\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show(block=False)\n",
        "    plt.pause(1.0)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 8) Train in Two Phases w/Triplet Loss, capturing separate loss components\n",
        "#    plus adding test-set combined loss each epoch\n",
        "# ---------------------------------------------------------------------------\n",
        "albert_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "model = EnergyBasedTabTransformer(\n",
        "    albert_model,\n",
        "    numeric_dim=len(numeric_cols),\n",
        "    categorical_cols=categorical_cols,\n",
        "    cat_dims=cat_dims\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "num_epochs = 15\n",
        "margin_normal  = -0.1\n",
        "margin_anomaly =  0.1\n",
        "phase1_end     = 10\n",
        "\n",
        "lambda_cls_list     = []\n",
        "lambda_cov_list     = []\n",
        "lambda_anomaly_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch < phase1_end:\n",
        "        lambda_cls     = 0.5\n",
        "        lambda_cov     = 0.0\n",
        "        lambda_anomaly = 0.0\n",
        "    else:\n",
        "        lambda_cls     = 0.1\n",
        "        lambda_cov     = 0.1\n",
        "        lambda_anomaly = 0.3\n",
        "    lambda_cls_list.append(lambda_cls)\n",
        "    lambda_cov_list.append(lambda_cov)\n",
        "    lambda_anomaly_list.append(lambda_anomaly)\n",
        "\n",
        "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "lambda_triplet  = 0.5\n",
        "noise_std       = 0.01\n",
        "\n",
        "# Lists for plotting:\n",
        "epoch_total_losses       = []  # combined train\n",
        "epoch_classification_losses = []\n",
        "epoch_margin_losses      = []\n",
        "epoch_triplet_losses     = []\n",
        "epoch_test_losses        = []  # new list for test combined loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    classification_loss_sum = 0.0\n",
        "    margin_loss_sum   = 0.0\n",
        "    triplet_loss_sum  = 0.0\n",
        "\n",
        "    cov_vals, anom_vals = [], []\n",
        "    normal_e_vals, anomaly_e_vals = [], []\n",
        "\n",
        "    curr_lambda_cls     = lambda_cls_list[epoch]\n",
        "    curr_lambda_cov     = lambda_cov_list[epoch]\n",
        "    curr_lambda_anomaly = lambda_anomaly_list[epoch]\n",
        "\n",
        "    phase_label = \"1\" if epoch < phase1_end else \"2\"\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs} [Phase {phase_label}] \"\n",
        "          f\"λ_cls={curr_lambda_cls}, λ_cov={curr_lambda_cov}, λ_anomaly={curr_lambda_anomaly}, λ_triplet={lambda_triplet}\")\n",
        "\n",
        "    batch_count = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
        "        batch_count += 1\n",
        "\n",
        "        numeric = batch[\"numeric\"].to(device)\n",
        "        categorical = batch[\"categorical\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        energy_scores, logits, embeddings = model(numeric, categorical, return_cls_emb=True)\n",
        "\n",
        "        # Weighted CE\n",
        "        train_labels = train_dataset.labels.numpy()\n",
        "        class_counts = np.bincount(train_labels)\n",
        "        total_data   = len(train_labels)\n",
        "        num_classes  = len(class_counts)\n",
        "        weights = total_data / (num_classes * class_counts)\n",
        "        weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "        # We request separate L_cls, L_margin from combined_loss\n",
        "        combined_val, L_cls_val, L_margin_val = combined_loss(\n",
        "            energy_scores, logits, labels,\n",
        "            margin_normal=margin_normal,\n",
        "            margin_anomaly=margin_anomaly,\n",
        "            lambda_cls=curr_lambda_cls,\n",
        "            lambda_margin_normal=curr_lambda_cov,\n",
        "            lambda_margin_anomaly=curr_lambda_anomaly,\n",
        "            weight=weights,\n",
        "            return_components=True\n",
        "        )\n",
        "\n",
        "        # indices as 1D vectors (length = number of matches)\n",
        "        normal_indices  = (labels == 0).nonzero(as_tuple=False).flatten()\n",
        "        anomaly_indices = (labels == 1).nonzero(as_tuple=False).flatten()\n",
        "\n",
        "        trip_loss_val = torch.tensor(0.0, device=device)\n",
        "        if normal_indices.numel() > 0 and anomaly_indices.numel() > 0:\n",
        "            # (N_norm, D)\n",
        "            anchor_embeddings = embeddings.index_select(0, normal_indices)\n",
        "            # (N_norm, D)\n",
        "            positive_embeddings = anchor_embeddings + torch.randn_like(anchor_embeddings) * noise_std\n",
        "\n",
        "            # (N_anom, D)\n",
        "            negative_pool = embeddings.index_select(0, anomaly_indices)\n",
        "            # sample one negative per anchor\n",
        "            rand_idx = torch.randint(0, negative_pool.size(0),\n",
        "                                    (anchor_embeddings.size(0),), device=device)\n",
        "            negatives_sampled = negative_pool[rand_idx]  # (N_norm, D)\n",
        "\n",
        "            trip_loss_val = triplet_loss_fn(anchor_embeddings, positive_embeddings, negatives_sampled)\n",
        "\n",
        "        total_batch_loss = combined_val + lambda_triplet * trip_loss_val\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "        classification_loss_sum += L_cls_val\n",
        "        margin_loss_sum         += L_margin_val\n",
        "        triplet_loss_sum        += trip_loss_val.item()\n",
        "\n",
        "        # track margin stats for debugging\n",
        "        normal_mask = (labels==0)\n",
        "        anomaly_mask= (labels==1)\n",
        "        if normal_mask.sum()>0:\n",
        "            L_cov_batch = F.relu(energy_scores[normal_mask] - margin_normal).mean().item()\n",
        "            normal_e_vals.append(energy_scores[normal_mask].mean().item())\n",
        "        else:\n",
        "            L_cov_batch = 0.0\n",
        "            normal_e_vals.append(0.0)\n",
        "\n",
        "        if anomaly_mask.sum()>0:\n",
        "            L_anom_batch = F.relu(margin_anomaly - energy_scores[anomaly_mask]).mean().item()\n",
        "            anomaly_e_vals.append(energy_scores[anomaly_mask].mean().item())\n",
        "        else:\n",
        "            L_anom_batch = 0.0\n",
        "            anomaly_e_vals.append(0.0)\n",
        "\n",
        "        cov_vals.append(L_cov_batch)\n",
        "        anom_vals.append(L_anom_batch)\n",
        "\n",
        "    # Averages over training batches\n",
        "    avg_combined_loss = total_loss / batch_count\n",
        "    avg_cls_loss      = classification_loss_sum / batch_count\n",
        "    avg_margin_loss   = margin_loss_sum / batch_count\n",
        "    avg_trip_loss     = triplet_loss_sum / batch_count\n",
        "\n",
        "    avg_cov   = np.mean(cov_vals)\n",
        "    avg_anom  = np.mean(anom_vals)\n",
        "    avg_nrg_n = np.mean(normal_e_vals)\n",
        "    avg_nrg_a = np.mean(anomaly_e_vals)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Combined Loss: {avg_combined_loss:.4f}\")\n",
        "    print(f\"   => Classification Loss: {avg_cls_loss:.4f},  Margin Loss: {avg_margin_loss:.4f},  Triplet: {avg_trip_loss:.4f}\")\n",
        "    print(f\"   => Mean Cov Loss: {avg_cov:.4f}  Mean Anom Loss: {avg_anom:.4f}\")\n",
        "    print(f\"   => Mean E_norm: {avg_nrg_n:.4f}, E_anom: {avg_nrg_a:.4f}\")\n",
        "\n",
        "    # Visualization each epoch\n",
        "    visualize_in_2D(model, train_loader, epoch=epoch+1,\n",
        "                    margin_normal=margin_normal, margin_anomaly=margin_anomaly)\n",
        "\n",
        "    # Evaluate on test for metrics:\n",
        "    accuracy, precision, recall, f1, all_labels, all_preds, all_logits = evaluate_model(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1} - Test -> Acc={accuracy:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    # Also compute \"combined loss\" on test set (no triplet for test) to track\n",
        "    model.eval()\n",
        "    test_loss_sum = 0.0\n",
        "    test_batches  = 0\n",
        "    with torch.no_grad():\n",
        "        for b_test in test_loader:\n",
        "            test_batches += 1\n",
        "            numeric_t = b_test[\"numeric\"].to(device)\n",
        "            categorical_t = b_test[\"categorical\"].to(device)\n",
        "            labels_t = b_test[\"labels\"].to(device)\n",
        "\n",
        "            energy_t, logits_t = model(numeric_t, categorical_t)\n",
        "            # same weighting used for train or no weighting? We'll keep it consistent\n",
        "            test_loss_val = combined_loss(\n",
        "                energy_t, logits_t, labels_t,\n",
        "                margin_normal=margin_normal,\n",
        "                margin_anomaly=margin_anomaly,\n",
        "                lambda_cls=curr_lambda_cls,\n",
        "                lambda_margin_normal=curr_lambda_cov,\n",
        "                lambda_margin_anomaly=curr_lambda_anomaly,\n",
        "                weight=weights,\n",
        "                return_components=False\n",
        "            )\n",
        "            test_loss_sum += test_loss_val.item()\n",
        "\n",
        "    test_loss_avg = test_loss_sum / test_batches\n",
        "    print(f\"Epoch {epoch+1} - Test Combined Loss: {test_loss_avg:.4f}\")\n",
        "\n",
        "    # Store for plotting after\n",
        "    epoch_total_losses.append(avg_combined_loss)\n",
        "    epoch_classification_losses.append(avg_cls_loss)\n",
        "    epoch_margin_losses.append(avg_margin_loss)\n",
        "    epoch_triplet_losses.append(avg_trip_loss)\n",
        "    epoch_test_losses.append(test_loss_avg)\n",
        "\n",
        "# After training, let's plot all separate losses (train) + test combined\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs+1), epoch_total_losses, label='Train Combined Loss')\n",
        "plt.plot(range(1, num_epochs+1), epoch_classification_losses, label='Train Classification Loss')\n",
        "plt.plot(range(1, num_epochs+1), epoch_margin_losses, label='Train Margin Loss')\n",
        "plt.plot(range(1, num_epochs+1), epoch_triplet_losses, label='Train Triplet Loss')\n",
        "#plt.plot(range(1, num_epochs+1), epoch_test_losses, label='Test Combined Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# final ROC\n",
        "pos_probs = torch.softmax(torch.tensor(all_logits), dim=1).numpy()[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, pos_probs)\n",
        "auc_value = roc_auc_score(all_labels, pos_probs)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC (AUC={auc_value:.4f})\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"ROC Curve (Final Epoch)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "(accuracy, precision, recall, f1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9) Systematic Mutants: A1/A2 => want new label=anomaly, A3 => want stability\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def anomaly_injection_a1_a2(\n",
        "    X,\n",
        "    mutation_fraction=0.05,\n",
        "    min_mutations=2,\n",
        "    num_features=1,\n",
        "    intensity=\"moderate\"\n",
        "):\n",
        "    intensity_levels = {\n",
        "        \"weak\":[0.25,0.5,0.75],\n",
        "        \"moderate\":[1,1.5,2],\n",
        "        \"strong\":[8,9,10]\n",
        "    }\n",
        "    if intensity not in intensity_levels:\n",
        "        raise ValueError(\"Invalid intensity for A1/A2\")\n",
        "\n",
        "    all_num = X.select_dtypes(include=['number']).columns\n",
        "    numeric_columns = [c for c in all_num if c != \"Class\"]\n",
        "    if len(numeric_columns)<num_features:\n",
        "        raise ValueError(\"Not enough numeric columns for A1/A2\")\n",
        "\n",
        "    selected_features = random.sample(list(numeric_columns), num_features)\n",
        "    X = X.copy()\n",
        "    mutated_indices_set= set()\n",
        "    mutated_feature_names=[]\n",
        "\n",
        "    for feature in selected_features:\n",
        "        std_dev = X[feature].std()\n",
        "        mutation_factors = [m*std_dev for m in intensity_levels[intensity]]\n",
        "        mutation_factor  = np.random.choice(mutation_factors)\n",
        "\n",
        "        num_mutations = max(min_mutations,int(mutation_fraction*len(X)))\n",
        "        num_mutations = min(num_mutations, len(X))\n",
        "\n",
        "        mutated_rows = np.random.choice(X.index, size=num_mutations, replace=False)\n",
        "        mutated_indices_set.update(mutated_rows)\n",
        "\n",
        "        before_vals = X.loc[mutated_rows, feature].copy()\n",
        "        small_value = std_dev*0.025\n",
        "        X.loc[mutated_rows, feature] = np.where(\n",
        "            X.loc[mutated_rows, feature]==0,\n",
        "            small_value/10,\n",
        "            X.loc[mutated_rows, feature]\n",
        "        )\n",
        "        X.loc[mutated_rows, feature] += mutation_factor*np.sign(X.loc[mutated_rows, feature])\n",
        "        after_vals = X.loc[mutated_rows, feature].copy()\n",
        "\n",
        "        #print(f\"A1/A2 injected in {feature} factor={mutation_factor:.3f} (intensity={intensity})\")\n",
        "        #print(pd.DataFrame({\"Before\":before_vals,\"After\":after_vals}),\"\\n\")\n",
        "\n",
        "        mutated_feature_names.append(feature)\n",
        "\n",
        "    '''if len(selected_features)>1:\n",
        "        print(f\"A2 mutated columns (together): {mutated_feature_names}\\n\")\n",
        "    else:\n",
        "        print(f\"A1 mutated column: {mutated_feature_names}\\n\")'''\n",
        "\n",
        "    mutated_indices = sorted(list(mutated_indices_set))\n",
        "    return X, mutated_indices, mutated_feature_names\n",
        "\n",
        "def anomaly_injection_a3(\n",
        "    X,\n",
        "    mutation_fraction=0.05,\n",
        "    min_mutations=2,\n",
        "    num_features=1,\n",
        "    intensity=\"moderate\"\n",
        "):\n",
        "    intensity_levels = {\n",
        "        \"weak\":0.01,\n",
        "        \"moderate\":0.05,\n",
        "        \"strong\":1.0\n",
        "    }\n",
        "    if intensity not in intensity_levels:\n",
        "        raise ValueError(\"Invalid intensity for A3\")\n",
        "\n",
        "    noise_level = intensity_levels[intensity]\n",
        "    all_num = X.select_dtypes(include=['number']).columns\n",
        "    numeric_columns = [c for c in all_num if c != \"Class\"]\n",
        "    if len(numeric_columns)<num_features:\n",
        "        raise ValueError(\"Not enough numeric columns for A3\")\n",
        "\n",
        "    selected_features= random.sample(list(numeric_columns), num_features)\n",
        "    X = X.copy()\n",
        "    mutated_indices_set=set()\n",
        "\n",
        "    for feature in selected_features:\n",
        "        std_dev = X[feature].std()\n",
        "        num_mutations= max(min_mutations,int(mutation_fraction*len(X)))\n",
        "        num_mutations= min(num_mutations,len(X))\n",
        "\n",
        "        mutation_rows = np.random.choice(X.index,size=num_mutations,replace=False)\n",
        "        mutated_indices_set.update(mutation_rows)\n",
        "\n",
        "        before_vals = X.loc[mutation_rows,feature].values\n",
        "        noise = np.random.normal(loc=0, scale=noise_level*std_dev, size=num_mutations)\n",
        "        X.loc[mutation_rows,feature] = np.clip(\n",
        "            X.loc[mutation_rows,feature]+noise,\n",
        "            a_min=0,\n",
        "            a_max=None\n",
        "        )\n",
        "        after_vals = X.loc[mutation_rows,feature].values\n",
        "\n",
        "        #print(f\"A3 injected in {feature}, intensity={intensity}, noise_level={noise_level}, std={std_dev:.3f}\")\n",
        "        #print(\"Before:\\n\", before_vals)\n",
        "        #print(\"After:\\n\", after_vals,\"\\n\")\n",
        "\n",
        "    mutated_indices = sorted(list(mutated_indices_set))\n",
        "    return X, mutated_indices, selected_features\n",
        "\n",
        "def get_predictions_for_df(model, df_data):\n",
        "    df_temp = df_data.reset_index(drop=True).copy()\n",
        "    for col in categorical_cols:\n",
        "        df_temp[col] = df_temp[col].apply(lambda x: cat_id_maps[col].get(x, 0))\n",
        "\n",
        "    from torch.utils.data import DataLoader\n",
        "    ds_temp = AnomalyTabularDataset(df_temp, numeric_cols, categorical_cols, target_col)\n",
        "    ld_temp = DataLoader(ds_temp, batch_size=64, collate_fn=tabular_collate_fn)\n",
        "\n",
        "    all_preds=[]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for b_test in ld_temp:\n",
        "            numeric_t = b_test[\"numeric\"].to(device)\n",
        "            categorical_t = b_test[\"categorical\"].to(device)\n",
        "            _, logits_t = model(numeric_t, categorical_t)\n",
        "            preds_t = torch.argmax(logits_t, dim=1)\n",
        "            all_preds.extend(preds_t.cpu().tolist())\n",
        "    return np.array(all_preds)\n",
        "\n",
        "def evaluate_mutation_a1_a2(df_mutated, mutated_indices, model):\n",
        "    from torch.utils.data import DataLoader\n",
        "    ds_mut = AnomalyTabularDataset(df_mutated, numeric_cols, categorical_cols, target_col)\n",
        "    ld_mut = DataLoader(ds_mut, batch_size=64, collate_fn=tabular_collate_fn)\n",
        "\n",
        "    acc, prec, rec, f1, _, mut_preds, _ = evaluate_model(model, ld_mut)\n",
        "    mut_preds_arr = get_predictions_for_df(model, df_mutated)\n",
        "\n",
        "    predicted_anomaly = 0\n",
        "    for row_idx in mutated_indices:\n",
        "        if mut_preds_arr[row_idx]==1:\n",
        "            predicted_anomaly +=1\n",
        "\n",
        "    total_mut = len(mutated_indices)\n",
        "    if total_mut>0:\n",
        "        mutation_score = 100.0 * predicted_anomaly / total_mut\n",
        "    else:\n",
        "        mutation_score=0.0\n",
        "\n",
        "    return acc, prec, rec, f1, mutation_score\n",
        "\n",
        "def evaluate_mutation_a3_stability(df_mutated, mutated_indices, orig_preds, model):\n",
        "    from torch.utils.data import DataLoader\n",
        "    ds_mut = AnomalyTabularDataset(df_mutated, numeric_cols, categorical_cols, target_col)\n",
        "    ld_mut = DataLoader(ds_mut, batch_size=64, collate_fn=tabular_collate_fn)\n",
        "\n",
        "    acc, prec, rec, f1, _, mut_preds, _ = evaluate_model(model, ld_mut)\n",
        "    mut_preds_arr = get_predictions_for_df(model, df_mutated)\n",
        "\n",
        "    changed = 0\n",
        "    for row_idx in mutated_indices:\n",
        "        if orig_preds[row_idx]!=mut_preds_arr[row_idx]:\n",
        "            changed+=1\n",
        "    total = len(mutated_indices)\n",
        "    if total>0:\n",
        "        stability_score=100.0*(total-changed)/total\n",
        "    else:\n",
        "        stability_score=100.0\n",
        "\n",
        "    return acc, prec, rec, f1, stability_score\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"MUTATION ANALYSIS (SYSTEMATIC MUTANTS)\")\n",
        "print(\"============================\")\n",
        "\n",
        "orig_preds_for_test = get_predictions_for_df(model, test_df)\n",
        "\n",
        "df_a1, mut_idxs_a1, feats_a1 = anomaly_injection_a1_a2(\n",
        "    X=test_df.copy(),\n",
        "    mutation_fraction=0.1,\n",
        "    min_mutations=2,\n",
        "    num_features=1,\n",
        "    intensity=\"strong\"\n",
        ")\n",
        "a1_acc, a1_prec, a1_rec, a1_f1, a1_score = evaluate_mutation_a1_a2(df_a1, mut_idxs_a1, model)\n",
        "print(f\"\\n--- A1: single feature shift -> want anomaly => 'mutation score' is % predicted=1 among mutated ---\")\n",
        "print(f\"Accuracy:        {a1_acc:.4f}\")\n",
        "print(f\"Precision:       {a1_prec:.4f}\")\n",
        "print(f\"Recall:          {a1_rec:.4f}\")\n",
        "print(f\"F1 Score:        {a1_f1:.4f}\")\n",
        "print(f\"Mutation Score:  {a1_score:.1f}%  (# predicted anomaly among mutated)\")\n",
        "\n",
        "df_a2, mut_idxs_a2, feats_a2 = anomaly_injection_a1_a2(\n",
        "    X=test_df.copy(),\n",
        "    mutation_fraction=0.1,\n",
        "    min_mutations=2,\n",
        "    num_features=3,\n",
        "    intensity=\"strong\"\n",
        ")\n",
        "a2_acc, a2_prec, a2_rec, a2_f1, a2_score = evaluate_mutation_a1_a2(df_a2, mut_idxs_a2, model)\n",
        "print(f\"\\n--- A2: multiple feature shift -> want anomaly => 'mutation score' is % predicted=1 among mutated ---\")\n",
        "print(f\"Accuracy:        {a2_acc:.4f}\")\n",
        "print(f\"Precision:       {a2_prec:.4f}\")\n",
        "print(f\"Recall:          {a2_rec:.4f}\")\n",
        "print(f\"F1 Score:        {a2_f1:.4f}\")\n",
        "print(f\"Mutation Score:  {a2_score:.1f}%  (# predicted anomaly among mutated)\")\n",
        "\n",
        "df_a3, mut_idxs_a3, feats_a3 = anomaly_injection_a3(\n",
        "    X=test_df.copy(),\n",
        "    mutation_fraction=0.1,\n",
        "    min_mutations=2,\n",
        "    num_features=2,\n",
        "    intensity=\"weak\"\n",
        ")\n",
        "a3_acc, a3_prec, a3_rec, a3_f1, a3_stability = evaluate_mutation_a3_stability(\n",
        "    df_a3, mut_idxs_a3, orig_preds_for_test, model\n",
        ")\n",
        "print(f\"\\n--- A3: random noise -> want no change => 'stability score' is % that kept same label ---\")\n",
        "print(f\"Accuracy:         {a3_acc:.4f}\")\n",
        "print(f\"Precision:        {a3_prec:.4f}\")\n",
        "print(f\"Recall:           {a3_rec:.4f}\")\n",
        "print(f\"F1 Score:         {a3_f1:.4f}\")\n",
        "print(f\"Stability Score:  {a3_stability:.1f}%  (unchanged preds among mutated)\")\n",
        "\n",
        "print(\"\\nMUTATION ANALYSIS COMPLETED.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747253143397
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "jY3hxVBrneSo"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}